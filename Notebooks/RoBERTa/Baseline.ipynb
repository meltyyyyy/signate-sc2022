{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    notebook = \"RoBERTa/Baseline\"\n",
    "    script = \"roberta/baseline\"\n",
    "    model = \"roberta-base\"\n",
    "\n",
    "    n_splits = 4\n",
    "    batch_size = 16\n",
    "    trn_fold = [0, 1, 2, 3]\n",
    "    # max length of token\n",
    "    max_len = 128\n",
    "    lr = 2e-5\n",
    "    \n",
    "    # optimizer settings\n",
    "    weight_decay = 2e-5\n",
    "    beta = (0.9, 0.98)\n",
    "    num_warmup_steps_rate = 0.01\n",
    "    clip_grad_norm = None\n",
    "    n_epochs = 10\n",
    "    gradient_accumulation_steps = 1\n",
    "    num_eval = 1\n",
    "    \n",
    "    seed = 42\n",
    "\n",
    "    # Reka Env\n",
    "    dir_path = \"/home/abe/kaggle/signate-sc2022\"\n",
    "\n",
    "    def is_notebook():\n",
    "        if 'get_ipython' not in globals():\n",
    "            return False\n",
    "        env_name = get_ipython().__class__.__name__  # type: ignore\n",
    "        if env_name == 'TerminalInteractiveShell':\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import random\n",
    "from glob import glob\n",
    "import subprocess\n",
    "from subprocess import PIPE\n",
    "import ntpath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-pastel')\n",
    "sns.set_palette(\"winter_r\")\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_setup(cfg):\n",
    "    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    cfg.INPUT = os.path.join(Config.dir_path, 'input')\n",
    "    cfg.OUTPUT = os.path.join(Config.dir_path, 'output')\n",
    "    cfg.SUBMISSION = os.path.join(Config.dir_path, 'submissions')\n",
    "    cfg.OUTPUT_EXP = os.path.join(cfg.OUTPUT, Config.script)\n",
    "    cfg.EXP_MODEL = os.path.join(cfg.OUTPUT_EXP, \"model\")\n",
    "    cfg.EXP_PREDS = os.path.join(cfg.OUTPUT_EXP, \"preds\")\n",
    "    cfg.EXP_FIG = os.path.join(cfg.OUTPUT_EXP, \"fig\")\n",
    "    cfg.NOTEBOOK = os.path.join(Config.dir_path, \"Notebooks\")\n",
    "    cfg.SCRIPT = os.path.join(Config.dir_path, \"scripts\")\n",
    "\n",
    "    # make dir\n",
    "    for dir in [\n",
    "            cfg.INPUT,\n",
    "            cfg.OUTPUT,\n",
    "            cfg.SUBMISSION,\n",
    "            cfg.OUTPUT_EXP,\n",
    "            cfg.EXP_MODEL,\n",
    "            cfg.EXP_PREDS,\n",
    "            cfg.EXP_FIG,\n",
    "            cfg.NOTEBOOK,\n",
    "            cfg.SCRIPT]:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    if Config.is_notebook():\n",
    "        notebook_path = os.path.join(cfg.NOTEBOOK, Config.notebook + \".ipynb\")\n",
    "        script_path = os.path.join(cfg.SCRIPT, Config.script + \".py\")\n",
    "        dir, _ = ntpath.split(script_path)\n",
    "        subprocess.run(f\"mkdir -p {dir}; touch {script_path}\",\n",
    "                       shell=True,\n",
    "                       stdout=PIPE,\n",
    "                       stderr=PIPE,\n",
    "                       text=True)\n",
    "        subprocess.run(\n",
    "            f\"jupyter nbconvert --to python {notebook_path} --output {script_path}\",\n",
    "            shell=True,\n",
    "            stdout=PIPE,\n",
    "            stderr=PIPE,\n",
    "            text=True)\n",
    "    \n",
    "    return cfg\n",
    "\n",
    "cfg = path_setup(Config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, cfg, texts, labels=None):\n",
    "        self.cfg = cfg\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.prepare_input(self.cfg, self.texts[index])\n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[index], dtype=torch.int64)\n",
    "            return inputs, label\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_input(cfg, text):\n",
    "        inputs = cfg.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=cfg.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=False,\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, cfg, criterion=None):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.criterion = criterion\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            cfg.model,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.backbone = AutoModel.from_pretrained(\n",
    "            cfg.model, \n",
    "            config=self.config\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 4),\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs, labels=None):\n",
    "        outputs = self.backbone(**inputs)[\"last_hidden_state\"]\n",
    "        outputs = outputs[:, 0, :]\n",
    "        if labels is not None:\n",
    "            logits = self.fc(outputs)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.fc(outputs)\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold\n",
    "def get_stratifiedkfold(train, target_col, n_splits, seed):\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "# collatte\n",
    "def collatte(inputs, labels=None):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    if not labels is None:\n",
    "        inputs = {\n",
    "            \"input_ids\" : inputs['input_ids'][:,:mask_len],\n",
    "            \"attention_mask\" : inputs['attention_mask'][:,:mask_len],\n",
    "        }\n",
    "        labels =  labels[:,:mask_len]\n",
    "        return inputs, labels, mask_len\n",
    "                \n",
    "    else:\n",
    "        inputs = {\n",
    "            \"input_ids\" : inputs['input_ids'][:,:mask_len],\n",
    "            \"attention_mask\" : inputs['attention_mask'][:,:mask_len],\n",
    "        }\n",
    "        return inputs, mask_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def training(cfg, train):\n",
    "    # =====================\n",
    "    # Training\n",
    "    # =====================\n",
    "    oof_pred = np.zeros((len(train), 4), dtype=np.float32)\n",
    "    \n",
    "    # 損失関数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for fold in cfg.trn_fold:\n",
    "        # Dataset,Dataloaderの設定\n",
    "        train_df = train.loc[cfg.folds!=fold]\n",
    "        valid_df = train.loc[cfg.folds==fold]\n",
    "        train_idx = list(train_df.index)\n",
    "        valid_idx = list(valid_df.index)\n",
    "\n",
    "        train_dataset = BERTDataset(\n",
    "            cfg,\n",
    "            train_df['description'].to_numpy(), \n",
    "            train_df['jobflag'].to_numpy(),\n",
    "        )\n",
    "        valid_dataset = BERTDataset(\n",
    "            cfg, \n",
    "            valid_df['description'].to_numpy(), \n",
    "            valid_df['jobflag'].to_numpy()\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        # 初期化\n",
    "        best_val_preds = None\n",
    "        best_val_score = -1\n",
    "\n",
    "        # modelの読み込み\n",
    "        model = BERTModel(cfg, criterion)\n",
    "        model = model.to(cfg.device)\n",
    "\n",
    "        # optimizer，schedulerの設定\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = []\n",
    "        optimizer_grouped_parameters.append({\n",
    "            'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "            'weight_decay': cfg.weight_decay\n",
    "        })\n",
    "        optimizer_grouped_parameters.append({\n",
    "            'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "            'weight_decay': 0.0\n",
    "        })\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=cfg.lr,\n",
    "            betas=cfg.beta,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "        )\n",
    "        num_train_optimization_steps = int(\n",
    "            len(train_loader) * cfg.n_epochs // cfg.gradient_accumulation_steps\n",
    "        )\n",
    "        num_warmup_steps = int(num_train_optimization_steps * cfg.num_warmup_steps_rate)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_train_optimization_steps\n",
    "        )\n",
    "        num_eval_step = len(train_loader) // cfg.num_eval + cfg.num_eval\n",
    "        \n",
    "        for epoch in range(cfg.n_epochs):\n",
    "            # training\n",
    "            print(f\"# ============ start epoch:{epoch} ============== #\")\n",
    "            model.train() \n",
    "            val_losses_batch = []\n",
    "            scaler = GradScaler()\n",
    "            with tqdm(train_loader, total=len(train_loader)) as pbar:\n",
    "                for step, (inputs, labels) in enumerate(pbar):\n",
    "                    inputs, max_len = collatte(inputs)\n",
    "                    for k, v in inputs.items():\n",
    "                        inputs[k] = v.to(cfg.device)\n",
    "                    labels = labels.to(cfg.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    with autocast():\n",
    "                        output, loss = model(inputs, labels)\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': loss.item(),\n",
    "                        'lr': scheduler.get_lr()[0]\n",
    "                    })\n",
    "\n",
    "                    if cfg.gradient_accumulation_steps > 1:\n",
    "                        loss = loss / cfg.gradient_accumulation_steps\n",
    "                    scaler.scale(loss).backward()\n",
    "                    if cfg.clip_grad_norm is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            model.parameters(), \n",
    "                            cfg.clip_grad_norm\n",
    "                        )\n",
    "                    if (step+1) % cfg.gradient_accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        scheduler.step()\n",
    "                \n",
    "            # evaluating\n",
    "            val_preds = []\n",
    "            val_losses = []\n",
    "            val_nums = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                with tqdm(valid_loader, total=len(valid_loader)) as pbar:\n",
    "                    for (inputs, labels) in pbar:\n",
    "                        inputs, max_len = collatte(inputs)\n",
    "                        for k, v in inputs.items():\n",
    "                            inputs[k] = v.to(cfg.device)\n",
    "                        labels = labels.to(cfg.device)\n",
    "                        with autocast():\n",
    "                            output, loss = model(inputs, labels)\n",
    "                        output = output.sigmoid().detach().cpu().numpy()\n",
    "                        val_preds.append(output)\n",
    "                        val_losses.append(loss.item() * len(labels))\n",
    "                        val_nums.append(len(labels))\n",
    "                        pbar.set_postfix({\n",
    "                            'val_loss': loss.item()\n",
    "                        })\n",
    "\n",
    "            val_preds = np.concatenate(val_preds)\n",
    "            val_loss = sum(val_losses) / sum(val_nums)\n",
    "            score = f1_score(np.argmax(val_preds, axis=1), valid_df['jobflag'], average='macro')\n",
    "            val_log = {\n",
    "                'val_loss': val_loss,\n",
    "                'score': score,\n",
    "            }\n",
    "            display(val_log)\n",
    "            if best_val_score < score:\n",
    "                print(\"save model weight\")\n",
    "                best_val_preds = val_preds\n",
    "                best_val_score = score\n",
    "                torch.save(\n",
    "                    model.state_dict(), \n",
    "                    os.path.join(cfg.EXP_MODEL, f\"fold{fold}.pth\")\n",
    "                )\n",
    "\n",
    "        oof_pred[valid_idx] = best_val_preds.astype(np.float32)\n",
    "        np.save(os.path.join(cfg.EXP_PREDS, f'oof_pred_fold{fold}.npy'), best_val_preds)\n",
    "        del model; gc.collect()\n",
    "\n",
    "    # scoring\n",
    "    np.save(os.path.join(cfg.EXP_PREDS, 'oof_pred.npy'), oof_pred)\n",
    "    score = f1_score(np.argmax(oof_pred, axis=1), train['jobflag'], average='macro')\n",
    "    print('CV:', round(score, 5))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferring(cfg, test):\n",
    "    print('\\n'.join(cfg.model_weights))\n",
    "    sub_pred = np.zeros((len(test), 4), dtype=np.float32)\n",
    "    for fold, model_weight in enumerate(cfg.model_weights):\n",
    "        # dataset, dataloader\n",
    "        test_dataset = BERTDataset(\n",
    "            cfg,\n",
    "            test['description'].to_numpy()\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset, \n",
    "            batch_size=cfg.batch_size, \n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        model = BERTModel(cfg)\n",
    "        model.load_state_dict(torch.load(model_weight))\n",
    "        model = model.to(cfg.device)\n",
    "\n",
    "        model.eval()\n",
    "        fold_pred = []\n",
    "        with torch.no_grad():\n",
    "            for inputs in tqdm(test_loader, total=len(test_loader)):\n",
    "                inputs, max_len = collatte(inputs)\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.to(cfg.device)\n",
    "                with autocast():\n",
    "                    output = model(inputs)\n",
    "                output = output.softmax(axis=1).detach().cpu().numpy()\n",
    "                fold_pred.append(output)\n",
    "        fold_pred = np.concatenate(fold_pred)\n",
    "        np.save(os.path.join(cfg.EXP_PREDS, f'sub_pred_fold{fold}.npy'), fold_pred)\n",
    "        sub_pred += fold_pred / len(cfg.model_weights)\n",
    "        del model; gc.collect()\n",
    "    np.save(os.path.join(cfg.EXP_PREDS, f'sub_pred.npy'), sub_pred)\n",
    "    return sub_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv(os.path.join(cfg.INPUT, 'train.csv'))\n",
    "\n",
    "# preprocess target\n",
    "train['jobflag'] -= 1\n",
    "\n",
    "# load tokenizer\n",
    "cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.model)\n",
    "# create folds\n",
    "cfg.folds = get_stratifiedkfold(train, 'jobflag', cfg.n_splits, cfg.seed)\n",
    "cfg.folds.to_csv(os.path.join(cfg.EXP_PREDS, 'folds.csv'))\n",
    "# train BERT\n",
    "score = training(cfg, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(cfg.INPUT, 'test.csv'))\n",
    "sub = pd.read_csv(os.path.join(cfg.INPUT, 'submit_sample.csv'), header=None)\n",
    "# BERTの推論\n",
    "cfg.model_weights = [p for p in sorted(glob(os.path.join(cfg.EXP_MODEL, 'fold*.pth')))]\n",
    "sub_pred = inferring(cfg, test)\n",
    "sub[1] = np.argmax(sub_pred, axis=1)\n",
    "sub[1] = sub[1].astype(int) + 1\n",
    "\n",
    "sub.to_csv(os.path.join(cfg.SUBMISSION, 'submission.csv'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('sc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec8c426eaf553261faa5dcddf5888c67da4c659d06ea0be71410dda2f5a31e25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
